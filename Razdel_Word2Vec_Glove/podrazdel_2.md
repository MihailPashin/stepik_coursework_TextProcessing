# Дистрибутивная Семантическая модель

_**Дистрибутивная Семантическая модель**_ 

- 1. [Варианты нейросетей](#1-варианты-нейросетей)
- 2. [Дистрибутивная семантика](#2-дистрибутивная-семантика)
- 3. [Glove](#3-glove)
- 4. [Word2Vec](#4-word2vec)
- 5. [Алгоритм Word2Vec](#5-алгоритм-word2vec)
- 6. [Развитие Word2Vec](#6-развитие-word2vec)
- 7. [Польза семантики](#7-польза-семантики)

---
## 1. Варианты нейросетей 

Повторим общий алгоритм обработки текста: Текст разбивается на базовые элементы, они нумеруются, по идентификаторам извлекаются строки весов из таблицы эмбеддинга.

Разновидности моделей:
-   Обучение нейросети end-to-end для конечной задачи(классификации). Требуются большой набор размеченных данных
-   Нейросети предсказывающие следующего слова. Моделирование языка. BERT и gpt'шка
-   Предсказание контекста употребления слова. Дистрибутивная семантика

Дистрибутивная семантика - рассчет вероятностей совместной встречаемости слов в рамках одного фрагмента текста
В центре области науки - поиск смыслообразования  слов за счет матрицы совместной встречаемости слов/токенов. Матрица встречаемости тяжела по размеру и объему. 

---
## 2. Дистрибутивная семантика
Матрица встречаемости как уже говорилось ранее носит недостатки. Есть возможность её упростить путем разбиения на две матрицы. Это Факторизация.

Сглаживание распределения - уменьшение диапозона всех значения. Нивелирование приоретета высокочастотных слов. Применяемый Способ -  Логарифмирования. Позволит прийти к порядкам а не абсолютным значения.

Способы упрощения матрицы совместной встречаемости:
 - Регрессия. Снижение средне-квадратичной ошибки
 - Классификация. Основывается на условных вероятностях
 - Сингулярное разложение. Три матрицы получаем. 2 Ортогональных 1 Диагональная

Первые два способа предпочтительны, их объединяет работа через градиентый спуск.
---

## 3. Glove
Glove -
Глобальные вектора, рассматривая глобальные контекста слов в рамках всего текста. Решают задачу регресии

Особенности метода:
- Сглаживания счетчика с помощью логарифма. Для сжатия диапазона значений
- Весовая функция - для сохранения специфической, но необходимой лексике


---
## 4. Word2Vec
Word2Vec -
моделирование распределение вероятности соседних слов. Простыми словами предсказания соседей при целевой фразе. Отличие проявляется в  Работе с малым локальным контекстом. Например, ширина окна 3

SkipGram - моделирование распределение соседей при условия наличия центра
CBOW - моделирование распределение центра при окружающих соседей

Для каждого слова назначают два вектора: для центрального расположения, для контекстного

Параметры настраюиваются градиентным спуском. Механизм схож с обучением нейросетей. Учим предсказанию

---
## 5. Алгоритм Word2Vec

SkipGram - моделирование распределение соседей при условия наличия центра. Обновляются веса согласно проходимым окнам по тексту.

Существует предположение условных независимости. Для последующей упрощения произведения вероятности.

Условная вероятность вычисляются по softmax, итерационно пробегаясь по всему корпусу текста на каждом шаге, это утомительно.

В противовес обычному softmax, выделяют алгоритм Negative Sampling.
 
Для расчет назначают случайно окно слов, а не весь корпус. Таким образом проводится апроксимация. 


---

## 6. Развитие Word2Vec

Важно приведение слов к нормальной форме. Для того, чтобы обойти высоко затратные методы лемматизации и стемминга, осуществляют
FastText - аналогичный Word2Vec'у алгоритм принимающий n-граммы символов. 

Это улучшит распознование однокоренных слов

---


## 7. Польза семантики

Возможно выявить схожие по смыслу слова, и кластеризовать по функциональности согласно предметной области.



---




