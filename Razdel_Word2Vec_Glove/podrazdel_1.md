# Общий алгоритм работы с текстами с помощью нейросетей

_**Общий алгоритм работы с текстами с помощью нейросетей**_ 

- 1. [Предобработка текста](#1-предобработка-текста)
   - 1.1. [Очищение текста от нерелевантных элементов](#очищение-текста-от-нерелевантных-элементов)
   - 1.2. [Построение словаря](#построение-словаря)
   - 1.3. [Текст в список номеров](#текст-в-список-номеров)
- 2. [Первичная векторизация нейросетями](#2-первичная-векторизация-нейросетями)
   - 2.1. [Таблица представления](#таблица-представления)
   - 2.2. [Преобразование тензора](#преобразование-тензора)
   - 2.3. [Агрегация тензора](#агрегация-тензора) 
- 3. [Поставленные задачи](#3-поставленные-задачи)

---
## 1. Предобработка текста 

### Очищение текста от нерелевантных элементов
Этап - _Токенизация_. Разбиение на токены и удаление оформления текста.  Результат шага - нормализованный список элементов
### Построение словаря
Составляется словарь с уникальными токенами/символы и назначаются им идентификаторы. Результат шага - _пронумерованный словарь токенов_ 
### Текст в список номеров
Оставляют только номера-идентификаторы символов и токенов. Результат шага - _список номеров_


---
## 2. Первичная векторизация нейросетями

### Таблица представления 
Построение Таблицы Эмбеддинга(векторного представления). Где Строки - уникальные токены из всего корпуса.  Столбцами отражают эмбеддинг. 
### Преобразование тензора
Тензор - матрица для прикладной задачи. Вносится контекст, которому служит отношение близлежайших токенов в корпусе текста.
Цель шага - _получение информации о локальном контексте_ 
### Агрегация тензора
Агрегация и упрощение. Получение обобщенного вектора из фрагмента текста. Это пригодится для оптимального выполнения задач классификации и анализа
Цель - _учет глобального контекста_

---
## 3. Поставленные задачи 

На втором семинаре будут рассматривать базовые механизмы нейросетей -  _векторизацию_, _учет локального контекста_, _агрегация_. В рамках задачи - построение Дистрибутивно-Семантической модели


---




