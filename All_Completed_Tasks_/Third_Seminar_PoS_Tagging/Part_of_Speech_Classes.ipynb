{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb62e4e3-09c5-4ba9-ad10-a08a76a79fe8",
   "metadata": {},
   "source": [
    "# Определение PartOfSpeech (частей речи) с помощью свёрточных нейросетей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d77ed8-6d08-469e-b71b-7711ddbd8d52",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb46c46-6843-4f68-956b-526abb11f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import wget\n",
    "import pyconll\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e9ae3-2345-4c92-a731-8811724a1528",
   "metadata": {},
   "source": [
    "Новая библиотека для загрузки корпуса Pyconll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957266d-7a94-4191-b674-3d421c583ae4",
   "metadata": {},
   "source": [
    "Датасет размеченный для распознавания PoS - SyntaGRus. Также может применяться для синтаксичего и морфологическего разбора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc4889-3163-4ca9-adb2-6278557efa9f",
   "metadata": {},
   "source": [
    "## Загружаем датасет "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c97d5ed-2164-4ab0-a252-284d1e4ea4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-03 15:51:23--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 40736599 (39M) [text/plain]\n",
      "Сохранение в: ‘../dataset/ru_syntagrus-ud-train.conllu’\n",
      "\n",
      "../dataset/ru_synta 100%[===================>]  38,85M  2,62MB/s    за 18s     \n",
      "\n",
      "2024-03-03 15:51:41 (2,19 MB/s) - ‘../dataset/ru_syntagrus-ud-train.conllu’ сохранён [40736599/40736599]\n",
      "\n",
      "--2024-03-03 15:51:41--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 14704579 (14M) [text/plain]\n",
      "Сохранение в: ‘../dataset/ru_syntagrus-ud-dev.conllu’\n",
      "\n",
      "../dataset/ru_synta 100%[===================>]  14,02M  2,84MB/s    за 4,7s    \n",
      "\n",
      "2024-03-03 15:51:46 (2,97 MB/s) - ‘../dataset/ru_syntagrus-ud-dev.conllu’ сохранён [14704579/14704579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ../dataset/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "!wget -O ../dataset/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7493de31-5893-4b51-acfe-cdb5ee946bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pyconll.load_from_file('../dataset/ru_syntagrus-ud-train.conllu')\n",
    "test_data = pyconll.load_from_file('../dataset/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34131144-a4de-4f3a-a427-de9a5267cc2e",
   "metadata": {},
   "source": [
    "Вид разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc9d41e-eb5c-4eef-a90d-9f55d76becb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in train_data[:1]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93556425-568f-415c-b773-bc17653d6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 194\n",
      "Наибольшая длина токена 31\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in train_data)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in train_data for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13a14f0-d65d-4b61-a51b-9896ac228c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in train_data]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533727de-d974-45ac-ba8f-b72f05abdfac",
   "metadata": {},
   "source": [
    "## Построение словаря символов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191126b0-4b67-4428-b918-53b2c0f03a6d",
   "metadata": {},
   "source": [
    "### Методы, написанные разработчиками курса. Токенизатор, и Сборщик словаря символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634ba48a-b899-4eeb-8098-876d3d33c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r'[\\w\\d]+') \n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
    "    #print(txt)\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "def character_tokenize(txt): \n",
    "    return list(txt)\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs): \n",
    "    #print(texts)\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956eba96-5776-4e9b-9e79-3ef2c88b8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод, написанный преподавателями курса\n",
    "def build_vocabulary(tokenized_texts, max_size=50000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
    "    word_counts = collections.defaultdict(int)\n",
    "    doc_n = 0\n",
    "\n",
    "    # посчитать количество документов, в которых употребляется каждое слово\n",
    "    # а также общее количество документов\n",
    "    for txt in tokenized_texts:\n",
    "        doc_n += 1\n",
    "        unique_text_tokens = set(txt)\n",
    "        for token in unique_text_tokens:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    # убрать слишком редкие и слишком частые слова\n",
    "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
    "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
    "\n",
    "    # отсортировать слова по убыванию частоты\n",
    "    sorted_word_counts = sorted(word_counts.items(),\n",
    "                                reverse=True,\n",
    "                                key=lambda pair: pair[1])\n",
    "\n",
    "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
    "    if pad_word is not None:\n",
    "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
    "\n",
    "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
    "    if len(word_counts) > max_size:\n",
    "        sorted_word_counts = sorted_word_counts[:max_size]\n",
    "\n",
    "    # нумеруем слова\n",
    "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
    "\n",
    "    # нормируем частоты слов\n",
    "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "\n",
    "    return word2id, word2freq\n",
    "\n",
    "\n",
    "PAD_TOKEN = '__PAD__'\n",
    "NUMERIC_TOKEN = '__NUMBER__'\n",
    "NUMERIC_RE = re.compile(r'^([0-9.,e+\\-]+|[mcxvi]+)$', re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612336d-86fa-4952-8acf-c241e5af14ce",
   "metadata": {},
   "source": [
    "### Формирование словаря символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e388de36-1397-418b-904c-79ed72c1f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных символов 142\n",
      "[('<PAD>', 0), (' ', 1), ('о', 2), ('е', 3), ('а', 4), ('т', 5), ('и', 6), ('н', 7), ('.', 8), ('с', 9)]\n"
     ]
    }
   ],
   "source": [
    "train_simvols_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
    "simvols_vocab, word_doc_freq = build_vocabulary(train_simvols_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
    "print(\"Количество уникальных символов\", len(simvols_vocab))\n",
    "print(list(simvols_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dedf1b-40b8-4741-8337-4190c4fb485d",
   "metadata": {},
   "source": [
    "### Нумерация частей речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c4117f1-a9fd-42a3-97d2-ad6dd59c712f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<NOTAG>': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'PUNCT': 13,\n",
       " 'SCONJ': 14,\n",
       " 'SYM': 15,\n",
       " 'VERB': 16,\n",
       " 'X': 17}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in train_data for token in sent if token.upos})\n",
    "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622956ad-f635-4258-897d-4567dc6d306b",
   "metadata": {},
   "source": [
    "### Преобразование корпуса данных в Тензор, для pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fd2060-04b0-401c-b191-db3bbc716982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# На вход получает список токенизированных предложений+Pos, список нумерованных символов, \n",
    "def pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n",
    "    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long) \n",
    "    # +2 отступ для понимания расположения позиции символа\n",
    "    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n",
    "\n",
    "    for sent_i, sent in enumerate(sentences): ## итерация по предложениям\n",
    "        for token_i, token in enumerate(sent): ## итерация по токенам\n",
    "            if token.form is None:\n",
    "                continue\n",
    "            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n",
    "            for char_i, char in enumerate(token.form):\n",
    "                inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)                \n",
    "                            \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac00350e-383d-4cf2-afa1-3ceb655ec06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels = pos_corpus_to_tensor(train_data, simvols_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "\n",
    "test_inputs, test_labels = pos_corpus_to_tensor(test_data, simvols_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc00d0bc-6de1-4531-8ee5-96ef1658aa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 38,  4, 25,  4, 11, 19,  7,  6, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  2, 23, 11,  4,  9,  5,  7,  2, 22,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[1][:2] # Первые два слова во втором преддожении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ec88cc-a15b-4ddb-a9f9-416ed182548d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  1,  8,  8, 12, 12,  4,  8,  1, 13, 16,  2,  8,  3,  3, 13, 16,  2,\n",
       "         8,  2,  8,  5,  3, 10, 16,  2,  8,  8,  2,  8, 13,  8, 13, 13,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1] # Целевое значение - Частей речи. Лишние ноли необходимы для выравнивания матриц, и распараллеливания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c82a3-92a7-43fe-be50-7ba78e92bb32",
   "metadata": {},
   "source": [
    "## Инициализация сверточной архитектуры PosTagger по примеру из семинара"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56e61f-92af-47c7-8ac0-1c4ccdcd2d0f",
   "metadata": {},
   "source": [
    "### Вспомогательная свёрточная cтруктура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97db32c9-075a-420a-9bc3-a1f46bb9d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedConv1d(nn.Module):\n",
    "    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(layers_n):\n",
    "            layers.append(nn.Sequential(\n",
    "                # размерность сверточного слоя -# Одномерная свёртка\n",
    "                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n",
    "                # Отключение \"перегруженных нейронов\" для равномерной нагрузки\n",
    "                nn.Dropout(dropout),\n",
    "                # Функция активации\n",
    "                nn.LeakyReLU()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    # Функция реализующая алгоритм. \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e016d7-68ce-4b71-8837-7f5844d57eeb",
   "metadata": {},
   "source": [
    "###  Cвёрточная cтруктура на уровне отдельных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cb073d-8758-4650-9413-a5f3c5d5a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Предсказание части речи на уровне отдельных токенов.\n",
    "\n",
    "class SingleTokenPOSTagger(nn.Module):    \n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        \n",
    "        features = self.backbone(char_embeddings)\n",
    "        \n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "        \n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177b660-c486-485c-a723-d2d9ac4b302e",
   "metadata": {},
   "source": [
    "## Применение PosTagger на уровне токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a95d132c-2830-40d2-a4c1-1759700dc1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 47314\n"
     ]
    }
   ],
   "source": [
    "single_token_model = SingleTokenPOSTagger(len(simvols_vocab), len(label2id),\n",
    "                                            embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\n",
    "print('Количество параметров', sum(np.prod(t.shape) for t in single_token_model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0657df-b6b5-40b7-a33c-736d00f243c6",
   "metadata": {},
   "source": [
    "### Методы из библиотек курса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0767ebb7-14d5-4682-97e8-b023f5e39bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import traceback\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
    "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
    "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
    "                    max_batches_per_epoch_train=10000,\n",
    "                    max_batches_per_epoch_val=1000,\n",
    "                    data_loader_ctor=DataLoader,\n",
    "                    optimizer_ctor=None,\n",
    "                    lr_scheduler_ctor=None,\n",
    "                    shuffle_train=True,\n",
    "                    dataloader_workers_n=0):\n",
    "    \"\"\"\n",
    "    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n",
    "    :param model: torch.nn.Module - обучаемая модель\n",
    "    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n",
    "    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n",
    "    :param criterion: функция потерь для настройки модели\n",
    "    :param lr: скорость обучения\n",
    "    :param epoch_n: максимальное количество эпох\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
    "        отсутствие улучшения модели, чтобы обучение продолжалось.\n",
    "    :param l2_reg_alpha: коэффициент L2-регуляризации\n",
    "    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n",
    "    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n",
    "    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n",
    "        (по умолчанию torch.utils.data.DataLoader)\n",
    "    :return: кортеж из двух элементов:\n",
    "        - среднее значение функции потерь на валидации на лучшей эпохе\n",
    "        - лучшая модель\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_ctor is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
    "    else:\n",
    "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
    "\n",
    "    if lr_scheduler_ctor is not None:\n",
    "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
    "                                        num_workers=dataloader_workers_n)\n",
    "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                      num_workers=dataloader_workers_n)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_i = 0\n",
    "    best_model = copy.deepcopy(model)\n",
    "\n",
    "    for epoch_i in range(epoch_n):\n",
    "        try:\n",
    "            epoch_start = datetime.datetime.now()\n",
    "            print('Эпоха {}'.format(epoch_i))\n",
    "\n",
    "            model.train()\n",
    "            mean_train_loss = 0\n",
    "            train_batches_n = 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "                if batch_i > max_batches_per_epoch_train:\n",
    "                    break\n",
    "\n",
    "                batch_x = copy_data_to_device(batch_x, device)\n",
    "                batch_y = copy_data_to_device(batch_y, device)\n",
    "\n",
    "                pred = model(batch_x)\n",
    "                loss = criterion(pred, batch_y)\n",
    "\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                mean_train_loss += float(loss)\n",
    "                train_batches_n += 1\n",
    "\n",
    "            mean_train_loss /= train_batches_n\n",
    "            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n",
    "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
    "            print('Среднее значение функции потерь на обучении', mean_train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            mean_val_loss = 0\n",
    "            val_batches_n = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
    "                    if batch_i > max_batches_per_epoch_val:\n",
    "                        break\n",
    "\n",
    "                    batch_x = copy_data_to_device(batch_x, device)\n",
    "                    batch_y = copy_data_to_device(batch_y, device)\n",
    "\n",
    "                    pred = model(batch_x)\n",
    "                    loss = criterion(pred, batch_y)\n",
    "\n",
    "                    mean_val_loss += float(loss)\n",
    "                    val_batches_n += 1\n",
    "\n",
    "            mean_val_loss /= val_batches_n\n",
    "            print('Среднее значение функции потерь на валидации', mean_val_loss)\n",
    "\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_epoch_i = epoch_i\n",
    "                best_val_loss = mean_val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print('Новая лучшая модель!')\n",
    "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
    "                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n",
    "                    early_stopping_patience))\n",
    "                break\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step(mean_val_loss)\n",
    "\n",
    "            print()\n",
    "        except KeyboardInterrupt:\n",
    "            print('Досрочно остановлено пользователем')\n",
    "            break\n",
    "        except Exception as ex:\n",
    "            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n",
    "            break\n",
    "\n",
    "    return best_val_loss, best_model\n",
    "\n",
    "\n",
    "def predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n",
    "    \"\"\"\n",
    "    :param model: torch.nn.Module - обученная модель\n",
    "    :param dataset: torch.utils.data.Dataset - данные для применения модели\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :return: numpy.array размерности len(dataset) x *\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    results_by_batch = []\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        import tqdm\n",
    "        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n",
    "            batch_x = copy_data_to_device(batch_x, device)\n",
    "\n",
    "            if return_labels:\n",
    "                labels.append(batch_y.numpy())\n",
    "\n",
    "            batch_pred = model(batch_x)\n",
    "            results_by_batch.append(batch_pred.detach().cpu().numpy())\n",
    "\n",
    "    if return_labels:\n",
    "        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n",
    "    else:\n",
    "        return np.concatenate(results_by_batch, 0)\n",
    "        \n",
    "def copy_data_to_device(data, device):\n",
    "    if torch.is_tensor(data):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return [copy_data_to_device(elem, device) for elem in data]\n",
    "    raise ValueError('Недопустимый тип данных {}'.format(type(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd8cbc-f840-4270-8056-f8848a95eae6",
   "metadata": {},
   "source": [
    "### Обучение сверточной модели на уровне токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d55a944-cada-42ab-885f-641b95f6f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michalik/github/course_work/stepik_coursework_TextProcessing/Seminar_1_Classification/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 101 итераций, 155.47 сек\n",
      "Среднее значение функции потерь на обучении 0.2384020203337221\n",
      "Среднее значение функции потерь на валидации 0.07113874028667365\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 101 итераций, 153.16 сек\n",
      "Среднее значение функции потерь на обучении 0.05273136068688761\n",
      "Среднее значение функции потерь на валидации 0.04974862341169674\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 101 итераций, 155.23 сек\n",
      "Среднее значение функции потерь на обучении 0.04166504187454091\n",
      "Среднее значение функции потерь на валидации 0.04457950355983017\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(best_val_loss,\n",
    " best_single_token_model) = train_eval_loop(single_token_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            F.cross_entropy,\n",
    "                                            lr=5e-3,\n",
    "                                            epoch_n=3,\n",
    "                                            batch_size=64,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=5,\n",
    "                                            max_batches_per_epoch_train=100,\n",
    "                                            max_batches_per_epoch_val=100,\n",
    "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                           factor=0.5,verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c33252f3-97db-46f8-a43b-d6742c596ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [03:46,  3.39it/s]                                                        \n",
      "/tmp/ipykernel_4638/2011350103.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(train_labels))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.024675922468304634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   4330443\n",
      "         ADJ       0.86      0.91      0.89     43357\n",
      "         ADP       1.00      0.98      0.99     39344\n",
      "         ADV       0.85      0.84      0.85     22733\n",
      "         AUX       0.82      0.79      0.80      3537\n",
      "       CCONJ       0.88      0.98      0.93     15168\n",
      "         DET       0.71      0.91      0.80     10781\n",
      "        INTJ       1.00      0.08      0.15        50\n",
      "        NOUN       0.97      0.88      0.92    103538\n",
      "         NUM       0.93      0.87      0.90      5640\n",
      "        PART       0.97      0.76      0.85     13556\n",
      "        PRON       0.96      0.72      0.82     18734\n",
      "       PROPN       0.83      0.89      0.86     14854\n",
      "       PUNCT       1.00      1.00      1.00     77972\n",
      "       SCONJ       0.78      0.95      0.85      8057\n",
      "         SYM       1.00      0.99      0.99       420\n",
      "        VERB       0.83      0.96      0.89     47731\n",
      "           X       0.75      0.47      0.58       189\n",
      "\n",
      "    accuracy                           0.99   4756104\n",
      "   macro avg       0.90      0.83      0.84   4756104\n",
      "weighted avg       0.99      0.99      0.99   4756104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "279it [01:23,  3.32it/s]                                                        \n",
      "/tmp/ipykernel_4638/2011350103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(test_labels))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.028754806146025658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1574439\n",
      "         ADJ       0.83      0.90      0.86     15103\n",
      "         ADP       1.00      0.97      0.99     13717\n",
      "         ADV       0.81      0.81      0.81      7783\n",
      "         AUX       0.82      0.75      0.78      1390\n",
      "       CCONJ       0.89      0.98      0.93      5672\n",
      "         DET       0.71      0.86      0.78      4265\n",
      "        INTJ       1.00      0.04      0.08        24\n",
      "        NOUN       0.95      0.87      0.91     36238\n",
      "         NUM       0.84      0.81      0.83      1734\n",
      "        PART       0.96      0.73      0.83      5125\n",
      "        PRON       0.95      0.73      0.83      7444\n",
      "       PROPN       0.80      0.84      0.82      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.76      0.95      0.84      2865\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.81      0.95      0.88     17110\n",
      "           X       0.76      0.39      0.51       134\n",
      "\n",
      "    accuracy                           0.99   1727764\n",
      "   macro avg       0.88      0.80      0.81   1727764\n",
      "weighted avg       0.99      0.99      0.99   1727764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(single_token_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(single_token_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02a49d-ea84-41f4-88a3-f91c858db6b3",
   "metadata": {},
   "source": [
    "## Применение PosTagger на уровне предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a146e43-93cf-4e50-8ece-7416e56ee235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLevelPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
    "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        char_features = self.single_token_backbone(char_embeddings)\n",
    "        \n",
    "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "\n",
    "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
    "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "\n",
    "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125f59f-999d-4109-93ba-828a8410c400",
   "metadata": {},
   "source": [
    "### Обучение сверточной модели на уровне предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea06b06-15a6-43b5-951a-b5161a4fb0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 84370\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model = SentenceLevelPOSTagger(len(simvols_vocab), len(label2id), embedding_size=64,\n",
    "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
    "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
    "print('Количество параметров', sum(np.prod(t.shape) for t in sentence_level_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79313d7c-8ad6-4d13-a0af-07a5615cd2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michalik/github/course_work/stepik_coursework_TextProcessing/Seminar_1_Classification/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 101 итераций, 155.58 сек\n",
      "Среднее значение функции потерь на обучении 0.19894202418699122\n",
      "Среднее значение функции потерь на валидации 0.06185003311032116\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 101 итераций, 154.14 сек\n",
      "Среднее значение функции потерь на обучении 0.05146126087644313\n",
      "Среднее значение функции потерь на валидации 0.04147836896083733\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 101 итераций, 154.14 сек\n",
      "Среднее значение функции потерь на обучении 0.03901225387459934\n",
      "Среднее значение функции потерь на валидации 0.031702461717004825\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model) = train_eval_loop(sentence_level_model,\n",
    "                                              train_dataset,\n",
    "                                              test_dataset,\n",
    "                                              F.cross_entropy,\n",
    "                                              lr=5e-3,\n",
    "                                              epoch_n=3,\n",
    "                                              batch_size=64,\n",
    "                                              device='cuda',\n",
    "                                              early_stopping_patience=5,\n",
    "                                              max_batches_per_epoch_train=100,\n",
    "                                              max_batches_per_epoch_val=100,\n",
    "                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                           factor=0.5,verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cab2461-321a-43d7-bbf6-435d30ba95b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [03:37,  3.52it/s]                                                        \n",
      "/tmp/ipykernel_3181/4087871673.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(train_labels))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.029104648157954216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michalik/github/course_work/stepik_coursework_TextProcessing/Seminar_1_Classification/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/michalik/github/course_work/stepik_coursework_TextProcessing/Seminar_1_Classification/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/michalik/github/course_work/stepik_coursework_TextProcessing/Seminar_1_Classification/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   4330443\n",
      "         ADJ       0.82      0.89      0.85     43357\n",
      "         ADP       1.00      0.97      0.98     39344\n",
      "         ADV       0.80      0.70      0.75     22733\n",
      "         AUX       0.84      0.91      0.87      3537\n",
      "       CCONJ       0.88      0.98      0.93     15168\n",
      "         DET       0.84      0.83      0.83     10781\n",
      "        INTJ       0.00      0.00      0.00        50\n",
      "        NOUN       0.92      0.90      0.91    103538\n",
      "         NUM       0.90      0.73      0.81      5640\n",
      "        PART       0.96      0.72      0.83     13556\n",
      "        PRON       0.90      0.85      0.87     18734\n",
      "       PROPN       0.81      0.89      0.85     14854\n",
      "       PUNCT       1.00      1.00      1.00     77972\n",
      "       SCONJ       0.84      0.86      0.85      8057\n",
      "         SYM       1.00      0.95      0.97       420\n",
      "        VERB       0.83      0.91      0.87     47731\n",
      "           X       0.00      0.00      0.00       189\n",
      "\n",
      "    accuracy                           0.99   4756104\n",
      "   macro avg       0.80      0.78      0.79   4756104\n",
      "weighted avg       0.99      0.99      0.99   4756104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "279it [01:18,  3.54it/s]                                                        \n",
      "/tmp/ipykernel_3181/4087871673.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(test_labels))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.031789667904376984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michalik/github/course_work/stepik_coursework_TextProcessing/Seminar_1_Classification/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/michalik/github/course_work/stepik_coursework_TextProcessing/Seminar_1_Classification/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1574439\n",
      "         ADJ       0.79      0.87      0.83     15103\n",
      "         ADP       0.99      0.97      0.98     13717\n",
      "         ADV       0.76      0.67      0.71      7783\n",
      "         AUX       0.84      0.89      0.86      1390\n",
      "       CCONJ       0.89      0.98      0.93      5672\n",
      "         DET       0.83      0.78      0.80      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.90      0.89      0.89     36238\n",
      "         NUM       0.81      0.63      0.71      1734\n",
      "        PART       0.96      0.71      0.82      5125\n",
      "        PRON       0.89      0.83      0.86      7444\n",
      "       PROPN       0.80      0.87      0.83      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.82      0.85      0.84      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.82      0.90      0.86     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.99   1727764\n",
      "   macro avg       0.78      0.75      0.76   1727764\n",
      "weighted avg       0.99      0.99      0.99   1727764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michalik/github/course_work/stepik_coursework_TextProcessing/Seminar_1_Classification/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef743f33-e744-4bfe-92f1-7ebbe6f1ce50",
   "metadata": {},
   "source": [
    "## Класс POSTagger из библиотеки курса "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4eff65e2-832c-48c5-baf1-3afa0ccce36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger:\n",
    "    def __init__(self, model, char2id, id2label, max_sent_len, max_token_len):\n",
    "        self.model = model\n",
    "        self.char2id = char2id\n",
    "        self.id2label = id2label\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __call__(self, sentences):\n",
    "        tokenized_corpus = tokenize_corpus(sentences, min_token_size=1)\n",
    "\n",
    "        inputs = torch.zeros((len(sentences), self.max_sent_len, self.max_token_len + 2), dtype=torch.long)\n",
    "\n",
    "        for sent_i, sentence in enumerate(tokenized_corpus):\n",
    "            for token_i, token in enumerate(sentence):\n",
    "                for char_i, char in enumerate(token):\n",
    "                    inputs[sent_i, token_i, char_i + 1] = self.char2id.get(char, 0)\n",
    "\n",
    "        dataset = TensorDataset(inputs, torch.zeros(len(sentences)))\n",
    "        predicted_probs = predict_with_model(self.model, dataset)  # SentenceN x TagsN x MaxSentLen\n",
    "        predicted_classes = predicted_probs.argmax(1)\n",
    "\n",
    "        result = []\n",
    "        for sent_i, sent in enumerate(tokenized_corpus):\n",
    "            result.append([self.id2label[cls] for cls in predicted_classes[sent_i, :len(sent)]])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f2e9c-635e-4cbe-9e40-430ac53329bf",
   "metadata": {},
   "source": [
    "## Применение полученных теггеров и сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c1ee7b6-b52f-487c-9e53-9cde1d81d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_token_pos_tagger = POSTagger(single_token_model, simvols_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "sentence_level_pos_tagger = POSTagger(sentence_level_model, simvols_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5ab5884-e6b5-4fc6-b5b7-0dd5fe01717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок'\n",
    "]\n",
    "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed512ada-8e56-42f2-aabd-5b3bcbe9f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 13.95it/s]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "\n",
      "косил-VERB косой-ADJ косой-ADJ косой-ADJ\n",
      "\n",
      "глокая-ADJ куздра-NOUN штеко-DET будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "ведро-ADV дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n",
      "\n",
      "три-NUM да-NOUN три-NUM будет-VERB дырка-NOUN\n",
      "\n",
      "три-NUM да-NOUN три-NUM будет-VERB шесть-VERB\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23b54252-efab-4d74-af89-c8c43f7f9fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 15.47it/s]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "\n",
      "косил-VERB косой-ADJ косой-ADJ косой-ADJ\n",
      "\n",
      "глокая-ADJ куздра-NOUN штеко-ADV будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "ведро-ADV дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n",
      "\n",
      "три-NUM да-NOUN три-NUM будет-AUX дырка-NOUN\n",
      "\n",
      "три-NUM да-NOUN три-NUM будет-AUX шесть-VERB\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9c674-efe3-4de0-a319-af491ae33f32",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1607032-72eb-4c08-b8ed-cc18a3ab1a3d",
   "metadata": {},
   "source": [
    "Частеричная омономия осталась в той же форме и не была решена в полной мере. \n",
    "\n",
    "Однако величина метрики F1-Score на валидации, кроме особых случаев, вполне удовлетворительно и составляет\n",
    "для \n",
    "- Сверточной модели на уровне токенов = 81%\n",
    "- Сверточной модели на уровне предложений (бóльший учёт контекста) = 76%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
